{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from pathlib import Path \n",
    "os.chdir(Path('./..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.12.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_embeddings import MaskedLanguageModeling\n",
    "from unsupervised_embeddings import SimCSE\n",
    "from unsupervised_embeddings import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import csv\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = []\n",
    "# dev_dataset = []\n",
    "\n",
    "# with open('notebooks/AllNLI.tsv', 'rt', encoding='utf8') as f_eval:\n",
    "#     reader = csv.DictReader(f_eval, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "#     for row in reader:\n",
    "#         if row['split'] == 'train':\n",
    "#             train_dataset.append(row['sentence2'])\n",
    "#         elif row['split'] == 'dev':\n",
    "#             dev_dataset.append(row['sentence2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_file(name, inputs):\n",
    "#     with open('notebooks/'+name, 'wt', encoding='utf8') as f:\n",
    "#         for sample in inputs:\n",
    "#             f.write('{}\\n'.format(sample))\n",
    "\n",
    "# save_file('train_nli.csv', train_dataset)\n",
    "# save_file('dev_nli.csv', dev_dataset)\n",
    "\n",
    "# save_file('toy_train_nli.csv', train_dataset[:1000])\n",
    "# save_file('toy_dev_nli.csv', dev_dataset[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Windows/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      " 99%|█████████▉| 95/96 [00:11<00:00,  8.21it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 96/96 [00:12<00:00,  7.93it/s]\n",
      "Configuration saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.2249, 'train_samples_per_second': 245.4, 'train_steps_per_second': 7.853, 'train_loss': 2.0175697008768716, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\pytorch_model.bin\n",
      "tokenizer config file saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\tokenizer_config.json\n",
      "Special tokens file saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\special_tokens_map.json\n",
      "loading configuration file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23 were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\added_tokens.json. We won't load it.\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\vocab.txt\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\special_tokens_map.json\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\\tokenizer_config.json\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.23it/s]\n",
      "Configuration saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51/config.json\n",
      "Model weights saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51/pytorch_model.bin\n",
      "tokenizer config file saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51/tokenizer_config.json\n",
      "Special tokens file saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51/special_tokens_map.json\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.07it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.14it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.16it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.20it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.17it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.05it/s]\n",
      "Epoch: 100%|██████████| 7/7 [00:57<00:00,  8.20s/it]\n",
      "loading configuration file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_10-26-23\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\added_tokens.json. We won't load it.\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\vocab.txt\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\special_tokens_map.json\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_10-26-23-2022-06-23_10-30-51\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# mlm_epochs = 3\n",
    "# sim_cse_epochs = 7\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "\n",
    "# mlm = MaskedLanguageModeling(model_name, output_path=f'output/mlm_{mlm_epochs}')\n",
    "# mlm.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/toy_dev_nli.csv') \\\n",
    "#     .train(epochs=mlm_epochs, batch_size=32, info_steps=100) \\\n",
    "#         .save()\n",
    "\n",
    "# sim_cse = SimCSE(mlm.output_dir, output_path=f'output/mlm_{mlm_epochs}_sim_cse_{sim_cse_epochs}')\n",
    "# sim_cse.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/stsbenchmark.tsv') \\\n",
    "#     .train(epochs=sim_cse_epochs, batch_size=32, info_steps=100)\n",
    "\n",
    "\n",
    "model_dir = utils.consecutive_training( \n",
    "    train_path='notebooks/toy_train_nli.csv', \n",
    "    mlm_dev_path='notebooks/toy_dev_nli.csv',\n",
    "    sim_cse_dev_path='notebooks/stsbenchmark.tsv',\n",
    "    model_name='distilbert-base-uncased', \n",
    "    mlm_epochs=3, \n",
    "    sim_cse_epochs=7, \n",
    "    batch_size=32, \n",
    "    info_steps=100\n",
    ")\n",
    "\n",
    "utils.evaluate_embeddings(model_dir, 'notebooks/stsbenchmark.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4224a284912126f805ae0ee160b4a432bf2336f134b0772df0c6ad7ad36981c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
