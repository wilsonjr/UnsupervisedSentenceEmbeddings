{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from pathlib import Path \n",
    "os.chdir(Path('./..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.12.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_embeddings import MaskedLanguageModeling\n",
    "from unsupervised_embeddings import SimCSE\n",
    "from unsupervised_embeddings import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import csv\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = []\n",
    "# dev_dataset = []\n",
    "\n",
    "# with open('notebooks/AllNLI.tsv', 'rt', encoding='utf8') as f_eval:\n",
    "#     reader = csv.DictReader(f_eval, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "#     for row in reader:\n",
    "#         if row['split'] == 'train':\n",
    "#             train_dataset.append(row['sentence2'])\n",
    "#         elif row['split'] == 'dev':\n",
    "#             dev_dataset.append(row['sentence2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_file(name, inputs):\n",
    "#     with open('notebooks/'+name, 'wt', encoding='utf8') as f:\n",
    "#         for sample in inputs:\n",
    "#             f.write('{}\\n'.format(sample))\n",
    "\n",
    "# save_file('train_nli.csv', train_dataset)\n",
    "# save_file('dev_nli.csv', dev_dataset)\n",
    "\n",
    "# save_file('toy_train_nli.csv', train_dataset[:1000])\n",
    "# save_file('toy_dev_nli.csv', dev_dataset[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      " 99%|█████████▉| 95/96 [00:14<00:00,  7.77it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 96/96 [00:14<00:00,  6.58it/s]\n",
      "Configuration saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14.674, 'train_samples_per_second': 204.443, 'train_steps_per_second': 6.542, 'train_loss': 2.0175697008768716, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\pytorch_model.bin\n",
      "tokenizer config file saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\tokenizer_config.json\n",
      "Special tokens file saved in output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\special_tokens_map.json\n",
      "loading configuration file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57 were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\added_tokens.json. We won't load it.\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\vocab.txt\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\special_tokens_map.json\n",
      "loading file output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\\tokenizer_config.json\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.95it/s]\n",
      "Configuration saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25/config.json\n",
      "Model weights saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25/pytorch_model.bin\n",
      "tokenizer config file saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25/tokenizer_config.json\n",
      "Special tokens file saved in output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25/special_tokens_map.json\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.80it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.84it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.89it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.91it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.92it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.77it/s]\n",
      "Epoch: 100%|██████████| 7/7 [01:00<00:00,  8.58s/it]\n",
      "loading configuration file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"output/mlm_3/mlm_distilbert-base-uncased-2022-06-23_09-23-57\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\added_tokens.json. We won't load it.\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\vocab.txt\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\special_tokens_map.json\n",
      "loading file output/mlm_3_sim_cse_7/simcse_output_mlm_3_mlm_distilbert-base-uncased-2022-06-23_09-23-57-2022-06-23_09-27-25\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "mlm_epochs = 3\n",
    "sim_cse_epochs = 7\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "mlm = MaskedLanguageModeling(model_name, output_path=f'output/mlm_{mlm_epochs}')\n",
    "mlm.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/toy_dev_nli.csv') \\\n",
    "    .train(epochs=mlm_epochs, batch_size=32, info_steps=100) \\\n",
    "        .save()\n",
    "\n",
    "sim_cse = SimCSE(mlm.output_dir, output_path=f'output/mlm_{mlm_epochs}_sim_cse_{sim_cse_epochs}')\n",
    "sim_cse.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/stsbenchmark.tsv') \\\n",
    "    .train(epochs=sim_cse_epochs, batch_size=32, info_steps=100)\n",
    "\n",
    "utils.evaluate_embeddings(sim_cse.output_dir, 'notebooks/stsbenchmark.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4224a284912126f805ae0ee160b4a432bf2336f134b0772df0c6ad7ad36981c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
