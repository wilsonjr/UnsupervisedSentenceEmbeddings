{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from pathlib import Path \n",
    "os.chdir(Path('./..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.12.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsupervised_embeddings import MaskedLanguageModeling\n",
    "from unsupervised_embeddings import SimCSE\n",
    "from unsupervised_embeddings import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import csv\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = []\n",
    "# dev_dataset = []\n",
    "\n",
    "# with open('notebooks/AllNLI.tsv', 'rt', encoding='utf8') as f_eval:\n",
    "#     reader = csv.DictReader(f_eval, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "#     for row in reader:\n",
    "#         if row['split'] == 'train':\n",
    "#             train_dataset.append(row['sentence2'])\n",
    "#         elif row['split'] == 'dev':\n",
    "#             dev_dataset.append(row['sentence2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_file(name, inputs):\n",
    "#     with open('notebooks/'+name, 'wt', encoding='utf8') as f:\n",
    "#         for sample in inputs:\n",
    "#             f.write('{}\\n'.format(sample))\n",
    "\n",
    "# save_file('train_nli.csv', train_dataset)\n",
    "# save_file('dev_nli.csv', dev_dataset)\n",
    "\n",
    "# save_file('toy_train_nli.csv', train_dataset[:1000])\n",
    "# save_file('toy_dev_nli.csv', dev_dataset[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm = MaskedLanguageModeling('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      " 31%|███▏      | 30/96 [00:06<00:08,  7.49it/s]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2975, 'learning_rate': 3.4375e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 31%|███▏      | 30/96 [00:06<00:08,  7.49it/s] Saving model checkpoint to output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-30\n",
      "Configuration saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-30\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4544107913970947, 'eval_runtime': 0.135, 'eval_samples_per_second': 740.615, 'eval_steps_per_second': 96.28, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-30\\pytorch_model.bin\n",
      " 62%|██████▎   | 60/96 [00:13<00:04,  7.61it/s]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9673, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 62%|██████▎   | 60/96 [00:13<00:04,  7.61it/s]Saving model checkpoint to output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-60\n",
      "Configuration saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-60\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.00439453125, 'eval_runtime': 0.1606, 'eval_samples_per_second': 622.566, 'eval_steps_per_second': 80.934, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-60\\pytorch_model.bin\n",
      "Deleting older checkpoint [output\\mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-30] due to args.save_total_limit\n",
      " 94%|█████████▍| 90/96 [00:20<00:00,  7.90it/s]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8952, 'learning_rate': 3.125e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 94%|█████████▍| 90/96 [00:20<00:00,  7.90it/s] Saving model checkpoint to output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-90\n",
      "Configuration saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-90\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2797064781188965, 'eval_runtime': 0.135, 'eval_samples_per_second': 740.729, 'eval_steps_per_second': 96.295, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-90\\pytorch_model.bin\n",
      "Deleting older checkpoint [output\\mlm_distilbert-base-uncased-2022-06-22_21-40-12\\checkpoint-60] due to args.save_total_limit\n",
      " 99%|█████████▉| 95/96 [00:23<00:00,  2.87it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 96/96 [00:24<00:00,  4.00it/s]\n",
      "Configuration saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.1793, 'train_samples_per_second': 124.073, 'train_steps_per_second': 3.97, 'train_loss': 2.036928335825602, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\pytorch_model.bin\n",
      "tokenizer config file saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\tokenizer_config.json\n",
      "Special tokens file saved in output/mlm_distilbert-base-uncased-2022-06-22_21-40-12\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unsupervised_embeddings.mlm.MaskedLanguageModeling at 0x22640d00c70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/toy_dev_nli.csv') \\\n",
    "    .train(epochs=3, batch_size=32, info_steps=30) \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at output/mlm_distilbert-base-uncased-2022-06-22_20-51-56 were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/mlm_distilbert-base-uncased-2022-06-22_20-51-56.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\added_tokens.json. We won't load it.\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\vocab.txt\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\special_tokens_map.json\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "sim_cse = SimCSE('output/mlm_distilbert-base-uncased-2022-06-22_20-51-56')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.94it/s]\n",
      "Configuration saved in output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55/config.json\n",
      "Model weights saved in output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55/pytorch_model.bin\n",
      "tokenizer config file saved in output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55/tokenizer_config.json\n",
      "Special tokens file saved in output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55/special_tokens_map.json\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  6.02it/s]\n",
      "Iteration: 100%|██████████| 32/32 [00:05<00:00,  5.94it/s]\n",
      "Epoch: 100%|██████████| 3/3 [00:25<00:00,  8.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unsupervised_embeddings.sim_cse.SimCSE at 0x22666b1d730>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_cse.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/stsbenchmark.tsv') \\\n",
    "    .train(epochs=3, batch_size=32, info_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at output/mlm_distilbert-base-uncased-2022-06-22_20-51-56 were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/mlm_distilbert-base-uncased-2022-06-22_20-51-56.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\added_tokens.json. We won't load it.\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\vocab.txt\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\special_tokens_map.json\n",
      "loading file output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model = models.Transformer('output/mlm_distilbert-base-uncased-2022-06-22_20-51-56', max_seq_length=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"output/mlm_distilbert-base-uncased-2022-06-22_20-51-56\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Didn't find file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\added_tokens.json. We won't load it.\n",
      "loading file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\vocab.txt\n",
      "loading file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\tokenizer.json\n",
      "loading file None\n",
      "loading file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\special_tokens_map.json\n",
      "loading file output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "utils.evaluate_embeddings('output/simcse_output_mlm_distilbert-base-uncased-2022-06-22_20-51-56-2022-06-22_21-48-55', 'notebooks/stsbenchmark.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_epochs = 3\n",
    "sim_cse_epochs = 7\n",
    "\n",
    "mlm = MaskedLanguageModeling('distilbert-base-uncased', output_path='output/mlm_3')\n",
    "mlm.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/toy_dev_nli.csv') \\\n",
    "    .train(epochs=mlm_epochs, batch_size=32, info_steps=100) \\\n",
    "        .save()\n",
    "\n",
    "sim_cse = SimCSE(mlm.output_dir, output_path='output/mlm_3_sim_cse_7')\n",
    "sim_cse.set_datasets('notebooks/toy_train_nli.csv', 'notebooks/stsbenchmark.tsv') \\\n",
    "    .train(epochs=3, batch_size=32, info_steps=100)\n",
    "\n",
    "utils.evaluate_embeddings(sim_cse.output_dir, 'notebooks/stsbenchmark.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4224a284912126f805ae0ee160b4a432bf2336f134b0772df0c6ad7ad36981c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
